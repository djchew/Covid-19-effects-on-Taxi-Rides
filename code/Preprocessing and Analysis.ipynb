{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc16537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shapefile\n",
    "from pandas.plotting import table\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from datetime import date\n",
    "from folium.plugins import FastMarkerCluster\n",
    "import os\n",
    "# a nice way of filtering out deprecated warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95213974",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = [\"march_taxi.csv\",\"april_taxi.csv\",\"may_taxi.csv\",\"june_taxi.csv\",\"july_taxi.csv\",\"august_taxi.csv\",\n",
    "         \"september_taxi.csv\", \"october_taxi.csv\", \n",
    "         \"november_taxi.csv\",\"december_taxi.csv\"]\n",
    "months = [\"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0df832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sf = gpd.read_file(\"../raw_data/taxi_zones.shp\")\n",
    "zone = pd.read_csv(\"../raw_data/taxi+_zone_lookup.csv\")\n",
    "\n",
    "# Convert the geometry shape to to latitude and longitude\n",
    "# Reproduced from Lab 2 for MAST30034\n",
    "sf['geometry'] = sf['geometry'].to_crs(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d734c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_number = [3,4,5,6,7,8,9,10,11,12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d70791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:06:25.517436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "for i in range(len(fname)):\n",
    "    df = pd.read_csv(\"../raw_data/\"+fname[i])\n",
    "   \n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "    # Code adapted from https://queirozf.com/entries/pandas-dataframe-examples-manipulating-date-and-time\n",
    "    if (fname[i] == \"march_taxi.csv\"):\n",
    "        df = df.loc[(pd.DatetimeIndex(df[\"tpep_pickup_datetime\"]).month == 3) & (pd.DatetimeIndex(df[\"tpep_pickup_datetime\"]).day >= 8)]\n",
    "    else:\n",
    "        df= df.loc[pd.DatetimeIndex(df[\"tpep_pickup_datetime\"]).month == month_number[i]]\n",
    "    df= df.loc[pd.DatetimeIndex(df[\"tpep_pickup_datetime\"]).year == 2020]\n",
    "    df[\"pickup_day\"] = df[\"tpep_pickup_datetime\"].map(lambda ts: ts.strftime(\"%m-%d-%y\"))\n",
    "\n",
    "    desc = df[[\"passenger_count\", \"trip_distance\"]].describe()\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    plt.figure(figsize = (15,15))\n",
    "    plt.title(\"Passenger Count according to Trip Distance\")\n",
    "    plt.xlabel(\"Trip Distance\")\n",
    "    plt.ylabel(\"Passenger Count\")\n",
    "    plt.scatter(df[[\"trip_distance\"]], df[[\"passenger_count\"]])                                 \n",
    "    \n",
    "    plt.savefig('../plots/'+months[i]+\"_desc_plot.png\")\n",
    "    plt.clf()\n",
    "    \n",
    "    ## Checked for valid trips since distance and fare amount has to be greater than 0 to count and pickup/dropoff locations have to be valid\n",
    "    df = df.loc[(df[\"passenger_count\"] > 0) & (df[\"trip_distance\"] < 2000) & (df[\"trip_distance\"] > 0) \n",
    "            & (df[\"fare_amount\"] < 1000) & (df[\"fare_amount\"] > 0) & (df[\"PULocationID\"] > 0) & (df[\"PULocationID\"] < 264)\n",
    "           & (df[\"DOLocationID\"] > 0) & (df[\"DOLocationID\"] < 264)]\n",
    "    df.to_csv(\"../preprocessed_data/\" + months[i] + \"_preprocess.csv\")\n",
    "    \n",
    "    ## Join data with their respective geographies\n",
    "    pickup_gdf = gpd.GeoDataFrame(pd.merge(df, sf, left_on='PULocationID', right_on='LocationID')).drop('PULocationID',axis=1)\n",
    "   \n",
    "    dropoff_gdf = gpd.GeoDataFrame(pd.merge(df, sf, left_on='DOLocationID', right_on='LocationID')).drop('DOLocationID',axis=1)\n",
    "    pickup_gdf['count'] = 1\n",
    "    dropoff_gdf['count'] = 1\n",
    "    \n",
    "    # Group data based on day and LocationID\n",
    "    rides_per_day = pickup_gdf.groupby([\"pickup_day\", \"LocationID\"]).count()[['count']].reset_index()\n",
    "    passenger_per_day = pickup_gdf.groupby([\"pickup_day\", \"LocationID\"]).mean()[['passenger_count']].reset_index()\n",
    "    distance_per_day = pickup_gdf.groupby([\"pickup_day\", \"LocationID\"]).mean()[['trip_distance']].reset_index()\n",
    "    \n",
    "    # Group data based on day and LocationID\n",
    "    rides_per_day = pickup_gdf.groupby([\"pickup_day\", \"LocationID\"]).count()[['count']].reset_index()\n",
    "    # Merged group day data with geodata\n",
    "    rides_per_day_merged = sf.set_index('LocationID').join(rides_per_day.set_index('LocationID'))\n",
    "    rides_per_day_merged = rides_per_day_merged.reset_index()\n",
    "    passenger_per_day_merged = sf.set_index('LocationID').join(passenger_per_day.set_index('LocationID'))\n",
    "    passenger_per_day_merged = passenger_per_day_merged.reset_index()\n",
    "    distance_per_day_merged = sf.set_index('LocationID').join(distance_per_day.set_index('LocationID'))\n",
    "    distance_per_day_merged = distance_per_day_merged.reset_index()\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "    # Get mean number of passengers per borough\n",
    "    passenger_per_day_per_borough = passenger_per_day_merged.groupby([\"pickup_day\",\"borough\"]).mean()[['passenger_count']].reset_index()\n",
    "    passenger_per_day_per_borough.to_csv(\"../preprocessed_data/\" + months[i] + \"_passengers.csv\")\n",
    "    \n",
    "    # Get count of pickups per borough\n",
    "    rides_per_day_per_borough = rides_per_day_merged.groupby([\"pickup_day\",\"borough\"]).sum()[['count']].reset_index()\n",
    "    rides_per_day_per_borough[\"pickup_day\"] = pd.to_datetime(rides_per_day_per_borough[\"pickup_day\"])\n",
    "    rides_per_day_per_borough.to_csv(\"../preprocessed_data/\" + months[i] + \"_rides_per_day.csv\")\n",
    "    \n",
    "    \n",
    "    pickup_geoJSON = pickup_gdf[['LocationID','geometry']].drop_duplicates('LocationID').to_json()\n",
    "    \n",
    "    # Get average distance per borough\n",
    "#     distance_merged = distance_merged.mean()[['trip_distance']].reset_index()\n",
    "    distance_per_day_per_borough = distance_per_day_merged.groupby([\"pickup_day\",\"borough\"]).mean()[['trip_distance']].reset_index()\n",
    "    # Get average rides per day per borough\n",
    "    distance_per_day_per_borough.to_csv(\"../preprocessed_data/\" + months[i] + \"_distance.csv\")\n",
    "    \n",
    "\n",
    "    # Draw folium chropleth maps\n",
    "    m_pickups = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "    # Adapted from lab 2 of MAST30034\n",
    "    # Plot pickup data\n",
    "    folium.Choropleth(\n",
    "        geo_data=pickup_geoJSON, # geoJSON \n",
    "        name='choropleth', # name of plot\n",
    "        data=rides_per_day, # data source\n",
    "        columns=['LocationID','count'], # the columns required\n",
    "        key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "        fill_color='OrRd', # color scheme\n",
    "        fill_opacity=0.9,\n",
    "        line_opacity=0.5,\n",
    "        legend_name='Trips' # legend title\n",
    "    ).add_to(m_pickups)\n",
    "\n",
    "    m_pickups.save('../plots/' + months[i] +'foliumChoroplethPickups.html')\n",
    "\n",
    "    m_pickups = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "    \n",
    "    folium.Choropleth(\n",
    "        geo_data=pickup_geoJSON, # geoJSON \n",
    "        name='choropleth', # name of plot\n",
    "        data=passenger_per_day, # data source\n",
    "        columns=['LocationID','passenger_count'], # the columns required\n",
    "        key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "        fill_color='OrRd', # color scheme\n",
    "        fill_opacity=0.9,\n",
    "        line_opacity=0.5,\n",
    "        legend_name='Passenger Count' # legend title\n",
    "    ).add_to(m_pickups)\n",
    "\n",
    "    m_pickups.save('../plots/' + months[i] + 'foliumChoroplethPassengers.html')\n",
    "    \n",
    "    m_pickups = folium.Map(location=[40.66, -73.94], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "\n",
    "    folium.Choropleth(\n",
    "        geo_data=pickup_geoJSON, # geoJSON \n",
    "        name='choropleth', # name of plot\n",
    "        data=distance_per_day, # data source\n",
    "        columns=['LocationID','trip_distance'], # the columns required\n",
    "        key_on='properties.LocationID', # this is from the geoJSON's properties\n",
    "        fill_color='OrRd', # color scheme\n",
    "        fill_opacity=0.9,\n",
    "        line_opacity=0.5,\n",
    "        legend_name='Trip Distance' # legend title\n",
    "    ).add_to(m_pickups)\n",
    "\n",
    "    m_pickups.save('../plots/' + months[i] + 'foliumChoroplethDistance.html')\n",
    " \n",
    " \n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9b1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
